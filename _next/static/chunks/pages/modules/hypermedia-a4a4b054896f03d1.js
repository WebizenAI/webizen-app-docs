(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[5453],{3464:function(e,n,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/modules/hypermedia",function(){return i(2311)}])},2311:function(e,n,i){"use strict";i.r(n),i.d(n,{__toc:function(){return d}});var s=i(2676),t=i(57),r=i(8949);let d=[{depth:2,value:"Purpose and Functionality",id:"purpose-and-functionality"},{depth:2,value:"Technical Implementation",id:"technical-implementation"},{depth:3,value:"Example Flow: Processing a New Video",id:"example-flow-processing-a-new-video"}];function o(e){let n=Object.assign({h1:"h1",p:"p",strong:"strong",h2:"h2",code:"code",ul:"ul",li:"li",a:"a",h3:"h3",ol:"ol"},(0,r.a)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{children:"Module: Hypermedia"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'The Hypermedia module is responsible for processing and enriching media files (audio and video) with a rich, semantic layer of metadata. It transforms standard media into "hypermedia," where the content is deeply indexed, searchable, and linked to other data in the user\'s knowledge graph.'})}),"\n",(0,s.jsx)(n.p,{children:"This module is central to Webizen's goal of creating a more intelligent and context-aware data environment."}),"\n",(0,s.jsx)(n.h2,{id:"purpose-and-functionality",children:"Purpose and Functionality"}),"\n",(0,s.jsxs)(n.p,{children:["The primary purpose of the ",(0,s.jsx)(n.code,{children:"hypermedia"})," module is to analyze and annotate media files to make their content more accessible and useful."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Core Functions:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Transcription:"})," It uses an AI speech-to-text engine (like a local Whisper model via the ",(0,s.jsx)(n.code,{children:"ai"})," module) to generate a time-coded transcript of any audio or video file. This transcript can then be translated into multiple languages by the ",(0,s.jsx)(n.code,{children:"translator"})," module."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SPARQL-MM Integration:"})," It uses the ",(0,s.jsx)(n.strong,{children:"SPARQL-MM"})," ontology (",(0,s.jsx)(n.code,{children:"ontologies/sparql-mm-v1.ttl"}),") to create detailed, timeline-based metadata. This includes:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice/Music Characteristics:"})," Identifying different speakers, types of music, or sound events."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Descriptions:"})," Tagging objects or concepts that appear or are mentioned at specific points in the media."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector Embedding:"})," The generated transcripts and metadata are processed by the ",(0,s.jsx)(n.code,{children:"ai"})," module to create vector embeddings, which are then stored in the ",(0,s.jsx)(n.code,{children:"vectordb"}),' (ChromaDB). This enables powerful semantic search over the content of the media (e.g., "find all videos where I discussed humanitarian logistics").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hypermedia Package Creation:"})," This module provides the core logic for bundling the original media file, its transcripts, timeline metadata, and other assets into a single, shareable ",(0,s.jsx)(n.a,{href:"../core-concepts/hypermedia-packages",children:"Hypermedia Content Package"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module Path"}),": ",(0,s.jsx)(n.code,{children:"src/modules/hypermedia/index.js"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"UI Component"}),": The interface for uploading, processing, and viewing hypermedia is rendered by ",(0,s.jsx)(n.code,{children:"src/components/Media.js"})," and ",(0,s.jsx)(n.code,{children:"src/components/Hypermedia.js"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Core Libraries"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Relies on AI services exposed by the ",(0,s.jsx)(n.code,{children:"ai"})," module."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ontology"}),": ",(0,s.jsx)(n.code,{children:"ontologies/sparql-mm-v1.ttl"})," is the primary schema for the generated metadata."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dependencies"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"modules/ai"}),": Essential for transcription, translation, and generating vector embeddings."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"modules/timeline"}),": For visualizing the time-coded metadata alongside the media playback."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"modules/vectordb"}),": To store the generated embeddings for semantic search."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"services/ipfs"}),": To store the final media files and hypermedia packages."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-flow-processing-a-new-video",children:"Example Flow: Processing a New Video"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["A user uploads a video file (",(0,s.jsx)(n.code,{children:"interview.mp4"}),") through the ",(0,s.jsx)(n.code,{children:"Media.js"})," component."]}),"\n",(0,s.jsxs)(n.li,{children:["The UI dispatches an event: ",(0,s.jsx)(n.code,{children:"eventBus.emit('hypermedia:process_file', { file: ... })"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"hypermedia"})," module's ",(0,s.jsx)(n.code,{children:"handleEvent"})," function receives the request."]}),"\n",(0,s.jsxs)(n.li,{children:["It sends a request to the ",(0,s.jsx)(n.code,{children:"ai"})," module to transcribe the audio from the video file."]}),"\n",(0,s.jsxs)(n.li,{children:["Once the transcript is returned, it sends further requests to the ",(0,s.jsx)(n.code,{children:"ai"})," module to:\na.  Translate the transcript into the user's preferred languages.\nb.  Analyze the transcript to identify key entities and concepts.\nc.  Generate vector embeddings for the transcript text."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"hypermedia"})," module constructs a detailed RDF graph using the SPARQL-MM ontology, linking the transcript, translations, and identified entities to specific timestamps in the video."]}),"\n",(0,s.jsxs)(n.li,{children:["This new RDF data is saved to the user's Solid Pod via the ",(0,s.jsx)(n.code,{children:"solidos.js"})," service."]}),"\n",(0,s.jsxs)(n.li,{children:["The vector embeddings are saved to the ",(0,s.jsx)(n.code,{children:"vectordb"})," module."]}),"\n",(0,s.jsx)(n.li,{children:"Finally, the user can choose to bundle all of these assets into a signed Hypermedia Content Package for sharing."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This process transforms a simple video file into a rich, searchable, and multilingual knowledge asset."})]})}n.default=(0,t.j)({MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,r.a)(),e.components);return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)},pageOpts:{filePath:"pages/modules/hypermedia.mdx",route:"/modules/hypermedia",timestamp:1752449798e3,title:"Module: Hypermedia",headings:d},pageNextRoute:"/modules/hypermedia"})}},function(e){e.O(0,[57,2888,9774,179],function(){return e(e.s=3464)}),_N_E=e.O()}]);