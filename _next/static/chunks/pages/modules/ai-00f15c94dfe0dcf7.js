(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[685],{8054:function(e,n,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/modules/ai",function(){return i(9287)}])},9287:function(e,n,i){"use strict";i.r(n),i.d(n,{__toc:function(){return l}});var s=i(2676),t=i(57),o=i(8949);let l=[{depth:2,value:"Purpose and Functionality",id:"purpose-and-functionality"},{depth:2,value:"Technical Implementation",id:"technical-implementation"},{depth:3,value:"Example Flow: AI-Assisted Email Response",id:"example-flow-ai-assisted-email-response"}];function r(e){let n=Object.assign({h1:"h1",p:"p",strong:"strong",h2:"h2",code:"code",ul:"ul",li:"li",h3:"h3",ol:"ol"},(0,o.a)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{children:"Module: Artificial Intelligence (AI)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The AI module is the central nervous system for all intelligent features within Webizen. It provides a unified interface for interacting with a wide range of AI models and services, both local and cloud-based."})}),"\n",(0,s.jsx)(n.p,{children:'This module is designed to be a flexible "router" for AI tasks, allowing the user and other modules to leverage powerful AI capabilities in a consistent and privacy-preserving manner.'}),"\n",(0,s.jsx)(n.h2,{id:"purpose-and-functionality",children:"Purpose and Functionality"}),"\n",(0,s.jsxs)(n.p,{children:["The primary purpose of the ",(0,s.jsx)(n.code,{children:"ai"})," module is to abstract the complexity of different AI providers and models, offering a single point of entry for tasks like text generation, text-to-speech, and semantic search."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Core Functions:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Routing:"})," It manages connections to multiple LLM backends, including:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local Models:"})," Ollama, LM Studio, and other local servers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud Models:"})," OpenAI, Grok, and Gemini.\nThe module routes requests to the appropriate model based on user configuration or the specific requirements of a task."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text-to-Speech (TTS) Management:"})," It integrates deeply with the ",(0,s.jsx)(n.strong,{children:"Chatterbox"})," engine (",(0,s.jsx)(n.code,{children:"modules/ai/chatterbox/"}),") as the default, high-quality TTS provider. It also manages connections to other services like the Web Speech API, Google Cloud TTS, and AWS Polly."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG):"})," For LLM queries, the module can interface with the ",(0,s.jsx)(n.code,{children:"vectordb"})," module to fetch relevant documents from the user's personal knowledge base. This context is then injected into the LLM prompt to provide more accurate and personalized responses."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cognitive AI Principles:"})," The module's design is guided by the principles of the W3C Cognitive AI Community Group (CogAI), aiming for transparency and explainability in AI-driven actions. Rule-based systems (",(0,s.jsx)(n.code,{children:"N3Logic"}),", ",(0,s.jsx)(n.code,{children:"RIF"}),") can be used alongside LLMs to provide a verifiable layer of reasoning."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module Path"}),": ",(0,s.jsx)(n.code,{children:"src/modules/ai/index.js"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sub-modules"}),": Contains specific implementations for different services, such as ",(0,s.jsx)(n.code,{children:"src/modules/ai/chatterbox/"})," and ",(0,s.jsx)(n.code,{children:"src/modules/ai/ollama/"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"UI Component"}),": The primary user interface for managing AI settings and interactions is ",(0,s.jsx)(n.code,{children:"src/components/AI.js"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration"}),": API keys, model endpoints, and user preferences are stored in ",(0,s.jsx)(n.code,{children:"config/webizen-config-v0.26.json"})," and ",(0,s.jsx)(n.code,{children:"ai/configs/"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dependencies"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"services/webizen-api"}),": Specifically the ",(0,s.jsx)(n.code,{children:"/ai/query"})," endpoint, which this module powers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"modules/vectordb"}),": For retrieving context for RAG."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"services/config"}),": To load API keys and model configurations."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"services/eventBus"}),": To listen for tasks that require AI processing."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-flow-ai-assisted-email-response",children:"Example Flow: AI-Assisted Email Response"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"email"})," module receives a new email and determines, based on a user-defined rule, that it requires an AI-generated draft response."]}),"\n",(0,s.jsxs)(n.li,{children:["It emits an event to the event bus: ",(0,s.jsx)(n.code,{children:"eventBus.emit('ai:generate_text_request', { task: 'email_draft', context: { originalEmail: '...' }, model: 'ollama' })"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"ai"})," module's ",(0,s.jsx)(n.code,{children:"handleEvent"})," function catches this request."]}),"\n",(0,s.jsxs)(n.li,{children:["It queries the ",(0,s.jsx)(n.code,{children:"vectordb"})," module with the content of the original email to find relevant past conversations or documents (RAG)."]}),"\n",(0,s.jsx)(n.li,{children:"It constructs a detailed prompt containing the original email text and the retrieved context."}),"\n",(0,s.jsxs)(n.li,{children:["It sends this prompt to the specified ",(0,s.jsx)(n.code,{children:"ollama"})," sub-module."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"ollama"})," sub-module makes the API call to the local Ollama server."]}),"\n",(0,s.jsxs)(n.li,{children:["Once the response is generated, the ",(0,s.jsx)(n.code,{children:"ai"})," module emits a new event: ",(0,s.jsx)(n.code,{children:"eventBus.emit('ai:generate_text_response', { task: 'email_draft', draft: '...' })"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"email"})," module listens for this event and populates the reply window with the AI-generated draft."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This workflow demonstrates how the ",(0,s.jsx)(n.code,{children:"ai"})," module acts as a powerful, centralized service, enabling other modules to easily integrate intelligent features without needing to know the specifics of any single AI model."]})]})}n.default=(0,t.j)({MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,o.a)(),e.components);return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(r,{...e})}):r(e)},pageOpts:{filePath:"pages/modules/ai.mdx",route:"/modules/ai",timestamp:1752449798e3,title:"Module: Artificial Intelligence (AI)",headings:l},pageNextRoute:"/modules/ai"})}},function(e){e.O(0,[57,2888,9774,179],function(){return e(e.s=8054)}),_N_E=e.O()}]);