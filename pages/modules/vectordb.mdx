# Module: Vector Database

**The Vector Database module provides the service for storing and querying vector embeddings. It is a crucial backend component that powers Webizen's semantic search capabilities and provides the long-term memory for the AI assistants.**

This module uses a local instance of ChromaDB to efficiently handle the high-dimensional data required for similarity searches.

## Purpose and Functionality

The primary goal of the `vectordb` module is to create a searchable, semantic index of the user's data. When content is processed, its meaning is converted into a numerical vector (an embedding); this module stores and queries those vectors.

**Core Functions:**
-   **Embedding Storage:** It provides a simple API for other modules (primarily the `ai` module) to add, update, and delete vector embeddings. Each embedding is stored with a link back to the original resource's URI.
-   **Similarity Search:** Its main function is to perform similarity searches. Given a query vector (representing the meaning of a search term), it returns a list of the most similar vectors from its database, allowing the application to find related content even if the keywords don't match exactly.
-   **Data Mapping:** It maintains a mapping between the vector embeddings and the user's primary RDF graph in Quadstore. This allows the system to retrieve the full, context-rich data for a resource found via a semantic search.
-   **Local-First AI:** By running ChromaDB locally, this module ensures that the semantic index of the user's personal data remains private and is never sent to a third-party service.

## Technical Implementation

-   **Module Path**: `src/modules/vectordb/index.js`
-   **Core Libraries**:
    -   `chromadb`: The Python-based vector database that is run as a background process. The module interacts with it via its client API.
-   **Ontology**: `ontologies/vectordb-v1.ttl` defines the properties used to link a resource in the main RDF graph to its corresponding vector ID in the ChromaDB instance.
-   **Dependencies**:
    -   `modules/ai`: The `ai` module is the primary consumer of this service. It generates the vector embeddings (using models like Ollama) and sends them to this module for storage. It also sends search queries to this module to power Retrieval-Augmented Generation (RAG).
    -   `modules/library`: The Semantic Library uses this module to provide its "search by meaning" functionality.
    -   `services/quadstore`: To link vector search results back to the rich metadata stored in the main RDF graph.

### Example Flow: Semantic Search for a Document

1.  A user types a query into the Semantic Library: "Information about fair-term contribution agreements."
2.  The `library` module sends this text query to the `ai` module.
3.  The `ai` module uses a text-embedding model to convert the query string into a vector embedding.
4.  The `ai` module then sends this query vector to the `vectordb` module's `search` function.
5.  The `vectordb` module performs a similarity search in its ChromaDB instance and finds the top 5 most similar vectors. It returns the URIs of the original documents associated with those vectors (e.g., `urn:ipfs:bafy...`, `https://user.pod.example/agreements/123`).
6.  The `library` module receives this list of URIs. For each URI, it fetches the full metadata from the **Quadstore** (e.g., title, author, creation date).
7.  The `library` component then displays the rich search results to the user, who has now found relevant documents without needing to match exact keywords.
